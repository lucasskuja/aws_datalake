{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbg4TxjoKZl5zyrV6ABoik",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucasskuja/aws_datalake/blob/main/extration_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Autor:** Lucas Skuja de Souza  \n",
        "\n",
        "## Resumo:\n",
        "Este projeto foi criado a fim de aprendizado ao criar uma pipeline de dados dento do ambiente AWS. Utilizamos nesse projeto as seguintes tecnologias:\n",
        "- AWS\n",
        "- Amazon S3\n",
        "- Python\n",
        "- Pandas\n",
        "\n",
        "\n",
        "## Introdução:\n",
        "Introdução detalhada sobre o que será abordado no notebook.\n",
        "\n",
        "## Dependências:\n",
        "```python\n",
        "# Instalar e importar bibliotecas necessárias\n",
        "!pip install apache-beam\n",
        "```\n",
        "## Referencias:\n",
        "- Esse projeto é baseado no curso: [AWS Data Lake: criando uma pipeline para ingestão de dados](https://cursos.alura.com.br/course/aws-data-lake-criando-pipeline-ingestao-dados)\n",
        "- Site [Data Boston](https://data.boston.gov/dataset/311-service-requests)\n"
      ],
      "metadata": {
        "id": "UDDVVaQ65si5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXgG2XYI5KEG",
        "outputId": "8fab3e7b-f8eb-443d-cae7-4fb434f496fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diretório 'data' já existe.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Verifica se o diretório \"data\" existe\n",
        "if not os.path.exists(\"data\"):\n",
        "  # Cria o diretório \"data\" se ele não existir\n",
        "  os.makedirs(\"data\")\n",
        "  print(\"Diretório 'data' criado com sucesso.\")\n",
        "else:\n",
        "  print(\"Diretório 'data' já existe.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "def extract_data(url, filename):\n",
        "    try:\n",
        "        print(f'Baixando {filename}')\n",
        "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "        response = requests.get(url, headers=headers, stream=True)\n",
        "        response.raise_for_status()  # Raise an exception for bad responses (4xx or 5xx)\n",
        "        with open(filename, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        time.sleep(5)  # Wait for 5 seconds after each download to avoid overwhelming server\n",
        "    except Exception as e:\n",
        "        print(f'Erro no download do {filename}')\n",
        "        print(f'ERRO:{e}')"
      ],
      "metadata": {
        "id": "hG8C0VfB7Vr8"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista dos nomes e urls dos arquivos baixados\n",
        "arquivos = {\n",
        "    'data/dados_2015.csv': 'https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/c9509ab4-6f6d-4b97-979a-0cf2a10c922b/download/tmphrybkxuh.csv',\n",
        "    'data/dados_2016.csv': 'https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/b7ea6b1b-3ca4-4c5b-9713-6dc1db52379a/download/tmpzxzxeqfb.csv',\n",
        "    'data/dados_2017.csv': 'https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/30022137-709d-465e-baae-ca155b51927d/download/tmpzccn8u4q.csv',\n",
        "    'data/dados_2018.csv': 'https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/2be28d90-3a90-4af1-a3f6-f28c1e25880a/download/tmp7602cia8.csv',\n",
        "    'data/dados_2019.csv': 'https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/ea2e4696-4a2d-429c-9807-d02eb92e0222/download/tmpcje3ep_w.csv',\n",
        "    'data/dados_2020.csv': 'https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/6ff6a6fd-3141-4440-a880-6f60a37fe789/download/tmpcv_10m2s.csv'\n",
        "}"
      ],
      "metadata": {
        "id": "mvRv2_SJ54qR"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extraindo os dados\n",
        "for k in arquivos:\n",
        "  extract_data(arquivos[k],k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrC193H77F-h",
        "outputId": "14039911-c942-470c-bb21-8816f18baa6d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baixando data/dados_2015.csv\n",
            "Baixando data/dados_2016.csv\n",
            "Baixando data/dados_2017.csv\n",
            "Baixando data/dados_2018.csv\n",
            "Baixando data/dados_2019.csv\n",
            "Baixando data/dados_2020.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Dicionário para armazenar os dados de cada arquivo\n",
        "dfs = {}\n",
        "\n",
        "# Loop para ler cada arquivo e adicionar ao dicionário\n",
        "for k in arquivos:\n",
        "    ano = k.split(\"_\")[-1].split(\".\")[0]  # Extrai o ano do nome do arquivo\n",
        "    print(ano)\n",
        "    print(k)\n",
        "    dfs[ano] = pd.read_csv(k) # Lê os dados do arquivo transforma em dataFrame e armazena no dicionário"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ8OF6D-MPz8",
        "outputId": "f8bb705d-19a2-4b1e-f7f7-03e50f7ae43e"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2015\n",
            "data/dados_2015.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-b0f3528809c8>:11: DtypeWarning: Columns (13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  dfs[ano] = pd.read_csv(k) # Lê os dados do arquivo transforma em dataFrame e armazena no dicionário\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2016\n",
            "data/dados_2016.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-b0f3528809c8>:11: DtypeWarning: Columns (13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  dfs[ano] = pd.read_csv(k) # Lê os dados do arquivo transforma em dataFrame e armazena no dicionário\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2017\n",
            "data/dados_2017.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-b0f3528809c8>:11: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  dfs[ano] = pd.read_csv(k) # Lê os dados do arquivo transforma em dataFrame e armazena no dicionário\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2018\n",
            "data/dados_2018.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-b0f3528809c8>:11: DtypeWarning: Columns (13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  dfs[ano] = pd.read_csv(k) # Lê os dados do arquivo transforma em dataFrame e armazena no dicionário\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2019\n",
            "data/dados_2019.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-b0f3528809c8>:11: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  dfs[ano] = pd.read_csv(k) # Lê os dados do arquivo transforma em dataFrame e armazena no dicionário\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2020\n",
            "data/dados_2020.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-b0f3528809c8>:11: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  dfs[ano] = pd.read_csv(k) # Lê os dados do arquivo transforma em dataFrame e armazena no dicionário\n"
          ]
        }
      ]
    }
  ]
}